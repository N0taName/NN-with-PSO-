{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PSO:\n",
    "    def __init__(self, particle_size, dimensions, inertia_weight=0.9, cognitive_coeff=1.5, social_coeff=1.5):\n",
    "        self.particle_size = particle_size\n",
    "        self.dimensions = dimensions\n",
    "        self.inertia_weight = inertia_weight\n",
    "        self.cognitive_coeff = cognitive_coeff\n",
    "        self.social_coeff = social_coeff\n",
    "        self.particles = torch.randn((particle_size, dimensions), device=device)\n",
    "        self.velocities = torch.zeros((particle_size, dimensions), device=device)\n",
    "        self.best_particles = self.particles.clone()\n",
    "        self.global_best = self.particles[0].clone()\n",
    "        self.best_scores = torch.full((particle_size,), float('inf'), device=device)\n",
    "\n",
    "    def update_particles(self, net, criterion, data_loader):\n",
    "        for i, particle in enumerate(self.particles):\n",
    "            net.load_state_dict(vec_to_state_dict(particle, net))\n",
    "            loss, _ = evaluate(net, criterion, data_loader)\n",
    "            if loss < self.best_scores[i]:\n",
    "                self.best_scores[i] = loss\n",
    "                self.best_particles[i] = particle.clone()\n",
    "                if loss < self.best_scores.min():\n",
    "                    self.global_best = particle.clone()\n",
    "\n",
    "            cognitive = self.cognitive_coeff * torch.rand(self.dimensions, device=device) * (self.best_particles[i] - particle)\n",
    "            social = self.social_coeff * torch.rand(self.dimensions, device=device) * (self.global_best - particle)\n",
    "            self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive + social\n",
    "            self.particles[i] += self.velocities[i]\n",
    "\n",
    "\n",
    "def vec_to_state_dict(vec, model):\n",
    "    state_dict = model.state_dict()\n",
    "    idx = 0\n",
    "    for param in state_dict:\n",
    "        numel = state_dict[param].numel()\n",
    "        state_dict[param] = vec[idx:idx+numel].reshape(state_dict[param].shape).to(device)\n",
    "        idx += numel\n",
    "    return state_dict\n",
    "\n",
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    model.train()\n",
    "    return total_loss / len(data_loader.dataset), 100 * correct / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_loader)):  \u001b[39m# Go through each batch in the loader\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> 19\u001b[0m     pso\u001b[39m.\u001b[39;49mupdate_particles(model, criterion, train_loader)\n\u001b[1;32m     20\u001b[0m     loss, accuracy \u001b[39m=\u001b[39m evaluate_model(model, criterion, train_loader)  \u001b[39m# Evaluate the current global best model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mPSO.update_particles\u001b[0;34m(self, net, criterion, data_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i, particle \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparticles):\n\u001b[1;32m     18\u001b[0m     net\u001b[39m.\u001b[39mload_state_dict(vec_to_state_dict(particle, net))\n\u001b[0;32m---> 19\u001b[0m     loss, _ \u001b[39m=\u001b[39m evaluate(net, criterion, data_loader)\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_scores[i]:\n\u001b[1;32m     21\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_scores[i] \u001b[39m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[26], line 46\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, data_loader)\u001b[0m\n\u001b[1;32m     44\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     45\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mfor\u001b[39;49;00m inputs, targets \u001b[39min\u001b[39;49;00m data_loader:\n\u001b[1;32m     47\u001b[0m         inputs, targets \u001b[39m=\u001b[39;49m inputs\u001b[39m.\u001b[39;49mto(device), targets\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     48\u001b[0m         outputs \u001b[39m=\u001b[39;49m model(inputs)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torchvision/transforms/functional.py:139\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mThis function does not support torchscript.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m    Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m--> 139\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n\u001b[1;32m    141\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torchvision/utils.py:582\u001b[0m, in \u001b[0;36m_log_api_usage_once\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, FunctionType):\n\u001b[1;32m    581\u001b[0m     name \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m--> 582\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_log_api_usage_once(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodule\u001b[39m}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FFNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the dimensions needed for PSO particles based on the model's parameters.\n",
    "dimensions = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "pso = PSO(particle_size=20, dimensions=dimensions)\n",
    "\n",
    "# Train and plot setup\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "\n",
    "# Train using PSO\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss, train_accuracy = [], []\n",
    "    pso.update_particles(model, criterion, train_loader)\n",
    "    loss, accuracy = evaluate_model(model, criterion, train_loader)  # Evaluate the current global best model\n",
    "    epoch_losses.append(loss)\n",
    "    epoch_accuracies.append(accuracy)\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    avg_accuracy = sum(epoch_accuracies) / len(epoch_accuracies)\n",
    "    print(f\"Epoch {epoch+1}: Avg Train Loss: {avg_loss:.4f}, Avg Train Accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_state_dict(vec_to_state_dict(pso.global_best, model))\n",
    "test_loss, test_accuracy = evaluate(model, criterion, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the loss and accuracy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroToML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
