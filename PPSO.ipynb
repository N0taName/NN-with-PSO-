{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PSO:\n",
    "    def __init__(self, particle_size, dimensions, inertia_weight=0.9, cognitive_coeff=1.5, social_coeff=1.5):\n",
    "        self.particle_size = particle_size\n",
    "        self.dimensions = dimensions\n",
    "        self.inertia_weight = inertia_weight\n",
    "        self.cognitive_coeff = cognitive_coeff\n",
    "        self.social_coeff = social_coeff\n",
    "        self.particles = torch.randn((particle_size, dimensions), device=device)\n",
    "        self.velocities = torch.zeros((particle_size, dimensions), device=device)\n",
    "        self.best_particles = self.particles.clone()\n",
    "        self.global_best = self.particles[0].clone()\n",
    "        self.best_scores = torch.full((particle_size,), float('inf'), device=device)\n",
    "\n",
    "    def update_particles(self, net, criterion, data_loader):\n",
    "        for i, particle in enumerate(self.particles):\n",
    "            net.load_state_dict(vec_to_state_dict(particle, net))\n",
    "            loss, _ = evaluate(net, criterion, data_loader)\n",
    "            if loss < self.best_scores[i]:\n",
    "                self.best_scores[i] = loss\n",
    "                self.best_particles[i] = particle.clone()\n",
    "                if loss < self.best_scores.min():\n",
    "                    self.global_best = particle.clone()\n",
    "\n",
    "            cognitive = self.cognitive_coeff * torch.rand(self.dimensions, device=device) * (self.best_particles[i] - particle)\n",
    "            social = self.social_coeff * torch.rand(self.dimensions, device=device) * (self.global_best - particle)\n",
    "            self.velocities[i] = self.inertia_weight * self.velocities[i] + cognitive + social\n",
    "            self.particles[i] += self.velocities[i]\n",
    "\n",
    "\n",
    "def vec_to_state_dict(vec, model):\n",
    "    state_dict = model.state_dict()\n",
    "    idx = 0\n",
    "    for param in state_dict:\n",
    "        numel = state_dict[param].numel()\n",
    "        state_dict[param] = vec[idx:idx+numel].reshape(state_dict[param].shape).to(device)\n",
    "        idx += numel\n",
    "    return state_dict\n",
    "\n",
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    model.train()\n",
    "    return total_loss / len(data_loader.dataset), 100 * correct / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Train Loss: 0.1121, Avg Train Accuracy: 10.84%\n",
      "Epoch 2: Avg Train Loss: 0.1694, Avg Train Accuracy: 10.42%\n",
      "Epoch 3: Avg Train Loss: 0.1869, Avg Train Accuracy: 10.47%\n",
      "Epoch 4: Avg Train Loss: 0.1789, Avg Train Accuracy: 9.80%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):  \u001b[39m# Run for 10 epochs\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     train_loss, train_accuracy \u001b[39m=\u001b[39m [], []\n\u001b[0;32m---> 16\u001b[0m     pso\u001b[39m.\u001b[39;49mupdate_particles(model, criterion, train_loader)\n\u001b[1;32m     17\u001b[0m     loss, accuracy \u001b[39m=\u001b[39m evaluate_model(model, criterion, train_loader)  \u001b[39m# Evaluate the current global best model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     epoch_losses\u001b[39m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mPSO.update_particles\u001b[0;34m(self, net, criterion, data_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i, particle \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparticles):\n\u001b[1;32m     18\u001b[0m     net\u001b[39m.\u001b[39mload_state_dict(vec_to_state_dict(particle, net))\n\u001b[0;32m---> 19\u001b[0m     loss, _ \u001b[39m=\u001b[39m evaluate(net, criterion, data_loader)\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_scores[i]:\n\u001b[1;32m     21\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_scores[i] \u001b[39m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[26], line 46\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, data_loader)\u001b[0m\n\u001b[1;32m     44\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     45\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mfor\u001b[39;49;00m inputs, targets \u001b[39min\u001b[39;49;00m data_loader:\n\u001b[1;32m     47\u001b[0m         inputs, targets \u001b[39m=\u001b[39;49m inputs\u001b[39m.\u001b[39;49mto(device), targets\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     48\u001b[0m         outputs \u001b[39m=\u001b[39;49m model(inputs)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/IntroToML/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FFNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the dimensions needed for PSO particles based on the model's parameters.\n",
    "dimensions = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "pso = PSO(particle_size=20, dimensions=dimensions)\n",
    "\n",
    "# Train and plot setup\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "\n",
    "# Train using PSO\n",
    "for epoch in range(10):  # Run for 10 epochs\n",
    "    train_loss, train_accuracy = [], []\n",
    "    pso.update_particles(model, criterion, train_loader)\n",
    "    loss, accuracy = evaluate_model(model, criterion, train_loader)  # Evaluate the current global best model\n",
    "    epoch_losses.append(loss)\n",
    "    epoch_accuracies.append(accuracy)\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    avg_accuracy = sum(epoch_accuracies) / len(epoch_accuracies)\n",
    "    print(f\"Epoch {epoch+1}: Avg Train Loss: {avg_loss:.4f}, Avg Train Accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "model.load_state_dict(vec_to_state_dict(pso.global_best, model))\n",
    "test_loss, test_accuracy = evaluate(model, criterion, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the loss and accuracy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroToML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
